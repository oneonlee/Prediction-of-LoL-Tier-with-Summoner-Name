# -*- coding: utf-8 -*-
"""Prediction_of_LoL_Tier_with_Summoner_Name.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xcIGEgBWajNtVOhVXVnF2EUP8beEHbt_

# 리그오브레전드 소환사 이름으로 티어 예측하기

## 필요 라이브러리 설치 및 불러오기

hgtk : https://github.com/bluedisk/hangul-toolkit
"""

# 한글 자모 단위 처리 패키지 설치
!pip install hgtk

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from tqdm import tqdm
import random
import hgtk
import urllib

"""## 데이터 내려받기 및 티어 나누기"""

urllib.request.urlretrieve("https://media.githubusercontent.com/media/oneonlee/Prediction-of-LoL-Tier-with-Summoner-Name/main/data/lol_raw_data.csv", filename="lol_raw_data.csv")

lol_raw_data = pd.read_csv("lol_raw_data.csv")

lol_raw_data

"""## EDA (Exploratory Data Analysis, 탐색적 데이터 분석)

`lol_raw_data`의 결측치 확인
"""

lol_raw_data.isnull().sum()

lol_raw_data.info()

print('소환사 이름의 최대 길이 :{}'.format(max(len(sample) for sample in lol_raw_data['nickname'])))
print('소환사 이름의 평균 길이 :{}'.format(sum(map(len, lol_raw_data['nickname']))/len(lol_raw_data['nickname'])))
print()

plt.figure(dpi=150)
plt.hist([len(sample) for sample in lol_raw_data['nickname']], bins=50)
plt.xlabel('length of nicknames')
plt.ylabel('frequencies')
plt.show()

sns.set(font_scale = 1.5)
fig, axe = plt.subplots(ncols=1)
fig.set_size_inches(16,8)
sns.countplot(x = lol_raw_data['tier'])

unique_elements, counts_elements = np.unique(lol_raw_data['tier'], return_counts=True)

for i in range(len(unique_elements)):
  print(f"{unique_elements[i]} tier : {counts_elements[i]}")

"""'master' 이상의 티어는 하나로 묶어서 다시 살펴보기"""

lol_edited_data = lol_raw_data.replace({'tier' : 'challenger'}, 'above_master')
lol_edited_data = lol_edited_data.replace({'tier' : 'grandmaster'}, 'above_master') 
lol_edited_data = lol_edited_data.replace({'tier' : 'master'}, 'above_master')

sns.set(font_scale = 1.5)
fig, axe = plt.subplots(ncols=1)
fig.set_size_inches(16,8)
sns.countplot(x = lol_edited_data['tier'])

edited_tier_list, counts_elements = np.unique(lol_edited_data['tier'], return_counts=True)

for i in range(len(edited_tier_list)):
  print(f"{edited_tier_list[i]} tier : {counts_elements[i]}")

"""## Over Sampling 수행"""

minimum = min(counts_elements)
min_idx = np.where(counts_elements==minimum)
least_tier = edited_tier_list[min_idx][0]

over_sampled_data = lol_edited_data[lol_edited_data['tier'] == least_tier]

over_sampled_data1 = over_sampled_data.copy()
for i in range(len(over_sampled_data1)):
    over_sampled_data1['nickname'][i] = over_sampled_data1['nickname'][i][1:]

over_sampled_data2 = over_sampled_data.copy()
for i in range(len(over_sampled_data2)):
    over_sampled_data2['nickname'][i] = over_sampled_data2['nickname'][i][:-1]

over_sampled_data = pd.concat([over_sampled_data, over_sampled_data1])
over_sampled_data = pd.concat([over_sampled_data, over_sampled_data2])

edited_tier_list

maximum = len(over_sampled_data)
edited_tier_list = np.delete(edited_tier_list, min_idx)

for tier in edited_tier_list:
  over_sampled_data = pd.concat([over_sampled_data, lol_edited_data[lol_edited_data['tier'] == tier].sample(maximum)])

over_sampled_data

X_data = over_sampled_data['nickname']
y_data = over_sampled_data['tier']

"""## LSTM으로 티어 예측하기"""

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, GRU, Bidirectional, Embedding
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import load_model

vocab_size = 1000
max_len = 16

"""티어의 정수 인코딩"""

y_data = y_data.replace('above_master', 0)
y_data = y_data.replace('diamond', 1)
y_data = y_data.replace('platinum', 2)
y_data = y_data.replace('gold', 3)
y_data = y_data.replace('silver', 4)
y_data = y_data.replace('bronze', 5)
y_data = y_data.replace('iron', 6)

"""### 자음 모음 단위 토큰화"""

def word_to_jamo(token):
  def to_special_token(jamo):
    if not jamo:
      return '-'
    else:
      return jamo

  decomposed_token = ''
  for char in token:
    try:
      # char(음절)을 초성, 중성, 종성으로 분리
      cho, jung, jong = hgtk.letter.decompose(char)

      # 자모가 빈 문자일 경우 특수문자 -로 대체
      cho = to_special_token(cho)
      jung = to_special_token(jung)
      jong = to_special_token(jong)
      decomposed_token = decomposed_token + cho + jung + jong

    # 만약 char(음절)이 한글이 아닐 경우 자모를 나누지 않고 추가
    except Exception as exception:
      if type(exception).__name__ == 'NotHangulException':
        decomposed_token += char
    
  # 단어 토큰의 자모 단위 분리 결과를 추가
  return decomposed_token

def jamo_to_word(jamo_sequence):
  tokenized_jamo = []
  index = 0
  
  # 1. 초기 입력
  # jamo_sequence = 'ㄴㅏㅁㄷㅗㅇㅅㅐㅇ'

  while index < len(jamo_sequence):
    # 문자가 한글(정상적인 자모)이 아닐 경우
    if not hgtk.checker.is_hangul(jamo_sequence[index]):
      tokenized_jamo.append(jamo_sequence[index])
      index = index + 1

    # 문자가 정상적인 자모라면 초성, 중성, 종성을 하나의 토큰으로 간주.
    else:
      tokenized_jamo.append(jamo_sequence[index:index + 3])
      index = index + 3

  # 2. 자모 단위 토큰화 완료
  # tokenized_jamo : ['ㄴㅏㅁ', 'ㄷㅗㅇ', 'ㅅㅐㅇ']
  
  word = ''
  try:
    for jamo in tokenized_jamo:

      # 초성, 중성, 종성의 묶음으로 추정되는 경우
      if len(jamo) == 3:
        if jamo[2] == "-":
          # 종성이 존재하지 않는 경우
          word = word + hgtk.letter.compose(jamo[0], jamo[1])
        else:
          # 종성이 존재하는 경우
          word = word + hgtk.letter.compose(jamo[0], jamo[1], jamo[2])
      # 한글이 아닌 경우
      else:
        word = word + jamo

  # 복원 중(hgtk.letter.compose) 에러 발생 시 초기 입력 리턴.
  # 복원이 불가능한 경우 예시) 'ㄴ!ㅁㄷㅗㅇㅅㅐㅇ'
  except Exception as exception:  
    if type(exception).__name__ == 'NotHangulException':
      return jamo_sequence

  # 3. 단어로 복원 완료
  # word : '남동생'

  return word

max_len = 16*3

characters_data = {}
jamorized_X_data = []

for X in tqdm(X_data):
    # 자음 모음 단위 토큰화
    jamo_X = list(word_to_jamo(X))
    result = []

    for jamo in jamo_X: 
        result.append(jamo)
        if jamo not in characters_data:
            characters_data[jamo] = 0 
        characters_data[jamo] += 1
    jamorized_X_data.append(result) 


print('단어 집합 :', characters_data)

data_vocab_sorted = sorted(characters_data.items(), key = lambda x:x[1], reverse = True)
print(data_vocab_sorted)

data_word_to_index = {}
i = 0
for (word, frequency) in data_vocab_sorted :
    if frequency > 1 : # 빈도수가 작은 단어는 제외.
        i = i + 1
        data_word_to_index[word] = i

print(data_word_to_index)

data_word_to_index['OOV'] = len(data_word_to_index) + 1
print(data_word_to_index)

encoded_X_data = []
for jamorized_X in tqdm(jamorized_X_data):
    encoded_sentence = []
    for jamo in jamorized_X:
        try:
            # 단어 집합에 있는 단어라면 해당 단어의 정수를 리턴.
            encoded_sentence.append(data_word_to_index[jamo])
        except KeyError:
            # 만약 단어 집합에 없는 단어라면 'OOV'의 정수를 리턴.
            encoded_sentence.append(data_word_to_index['OOV'])
    encoded_X_data.append(encoded_sentence)

encoded_X_data = pad_sequences(encoded_X_data, maxlen=max_len)

X_train, X_test, y_train, y_test = train_test_split(encoded_X_data, y_data, test_size=0.33, random_state=815, stratify=y_data)

print(f"전체 data의 개수 : {len(encoded_X_data)}")
print(f"train data의 개수 : {len(X_train)}")
print(f"test data의 개수 : {len(X_test)}")

"""#### LSTM ver.1 - *0.21945*"""

embedding_dim = 128
hidden_units = 128
num_classes = len(set(y_train))

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(LSTM(hidden_units))
model.add(Dense(num_classes, activation='softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('jamo_best_model_lstm_128.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, batch_size=128, epochs=30, 
                    callbacks=[es, mc],
                    validation_data=(X_test, y_test))

"""#### LSTM ver.2 - *0.21972*"""

embedding_dim = 64
hidden_units = 64
num_classes = len(set(y_train))

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(LSTM(hidden_units))
model.add(Dense(num_classes, activation='softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('jamo_best_model_lstm_64.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, batch_size=64, epochs=30, 
                    callbacks=[es, mc],
                    validation_data=(X_test, y_test))

"""#### Bi-LSTM ver.1 *0.22147*"""

embedding_dim = 128
hidden_units = 128
num_classes = len(set(y_train))

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(Bidirectional(LSTM(hidden_units)))
model.add(Dense(num_classes, activation='softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('jamo_best_model_bilstm_128.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, batch_size=128, epochs=30, 
                    callbacks=[es, mc],
                    validation_data=(X_test, y_test))

"""#### Bi-LSTM ver.2 *0.22132*"""

embedding_dim = 64
hidden_units = 64
num_classes = len(set(y_train))

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(Bidirectional(LSTM(hidden_units)))
model.add(Dense(num_classes, activation='softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('jamo_best_model_bilstm_64.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, batch_size=64, epochs=30, 
                    callbacks=[es, mc],
                    validation_data=(X_test, y_test))

"""#### GRU ver.1 *0.22105*"""

embedding_dim = 128
hidden_units = 128
num_classes = len(set(y_train))

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(GRU(hidden_units))
model.add(Dense(num_classes, activation='softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('jamo_best_model_gru_128.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, batch_size=128, epochs=30, 
                    callbacks=[es, mc],
                    validation_data=(X_test, y_test))

"""#### GRU ver.2 *0.21993*"""

embedding_dim = 64
hidden_units = 64
num_classes = len(set(y_train))

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(GRU(hidden_units))
model.add(Dense(num_classes, activation='softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('jamo_best_model_gru_64.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, batch_size=64, epochs=30, 
                    callbacks=[es, mc],
                    validation_data=(X_test, y_test))

"""#### Bi-GRU ver.1 *0.22192*"""

embedding_dim = 128
hidden_units = 128
num_classes = len(set(y_train))

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(Bidirectional(GRU(hidden_units)))
model.add(Dense(num_classes, activation='softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('jamo_best_model_biGRU_128.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, batch_size=128, epochs=30, 
                    callbacks=[es, mc],
                    validation_data=(X_test, y_test))

"""#### Bi-GRU ver.2 *0.22210*"""

embedding_dim = 64
hidden_units = 64
num_classes = len(set(y_train))

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(Bidirectional(GRU(hidden_units)))
model.add(Dense(num_classes, activation='softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('jamo_best_model_biGRU_64.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, batch_size=64, epochs=30, 
                    callbacks=[es, mc],
                    validation_data=(X_test, y_test))

"""### 모델 predict

가장 성능이 좋은 model을 `load` 한다.
"""

model = load_model('jamo_best_model_biGRU_64.h5')

# data_word_to_index = {'-': 1, 'ㅇ': 2, 'ㅏ': 3, 'ㄴ': 4, 'ㅣ': 5, 'ㄱ': 6, 'ㄹ': 7, 'ㅁ': 8, 'ㅗ': 9, ' ': 10, 'ㅅ': 11, 'ㅈ': 12, 'ㅓ': 13, 'ㅡ': 14, 'ㅜ': 15, 'ㅂ': 16, 'ㄷ': 17, 'ㅎ': 18, 'ㅐ': 19, 'a': 20, 'ㅕ': 21, 'ㅔ': 22, 'e': 23, 'i': 24, 'o': 25, 'ㅊ': 26, 'n': 27, 'ㅌ': 28, 'l': 29, 'ㅍ': 30, 'ㅋ': 31, 'r': 32, 's': 33, 'u': 34, '1': 35, 't': 36, 'ㅘ': 37, 'ㄲ': 38, 'h': 39, 'd': 40, 'ㅑ': 41, 'ㅛ': 42, 'I': 43, 'ㅠ': 44, 'g': 45, 'm': 46, 'y': 47, 'ㄸ': 48, 'k': 49, 'ㅝ': 50, '2': 51, '0': 52, 'c': 53, 'ㅆ': 54, 'ㅃ': 55, 'ㅉ': 56, 'ㅢ': 57, 'S': 58, 'A': 59, 'p': 60, 'w': 61, 'ㅟ': 62, 'T': 63, 'b': 64, 'O': 65, 'D': 66, 'E': 67, 'N': 68, 'G': 69, 'z': 70, 'f': 71, 'L': 72, 'R': 73, 'K': 74, 'M': 75, 'v': 76, '3': 77, 'ㅚ': 78, 'P': 79, 'C': 80, 'ㅖ': 81, 'x': 82, 'H': 83, '9': 84, 'j': 85, 'B': 86, '7': 87, '4': 88, '5': 89, '8': 90, 'J': 91, 'F': 92, '6': 93, 'Y': 94, 'W': 95, 'U': 96, 'q': 97, 'V': 98, 'X': 99, 'ㅄ': 100, 'Z': 101, 'ㅙ': 102, 'ㄺ': 103, 'ㄶ': 104, 'Q': 105, 'ㅞ': 106, 'ㅀ': 107, 'ㄻ': 108, 'ㅒ': 109, 'ㄼ': 110, 'ㄾ': 111, 'ㄵ': 112, 'ㄳ': 113, 'OOV': 114}

def tier_predict(nickname):
    jamo_nickname = word_to_jamo(nickname)
    encoded_nickname = []
    for jamo in jamo_nickname:
        try:
            # 단어 집합에 있는 단어라면 해당 단어의 정수를 리턴.
            encoded_nickname.append(data_word_to_index[jamo])
        except KeyError:
            # 만약 단어 집합에 없는 단어라면 'OOV'의 정수를 리턴.
            encoded_nickname.append(data_word_to_index['OOV'])
    padded_nickname = pad_sequences([encoded_nickname], maxlen=16)
    
    tier_index = np.argmax(model.predict(padded_nickname))
    tier_confidence = np.max(model.predict(padded_nickname))
    tier_list = ['마스터 이상', '다이아몬드', '플래티넘', '골드', '실버', '브론즈', '아이언']

    return tier_list[tier_index], tier_confidence

tier_predict('KT way')

tier_predict('플래티넘 문지기')

tier_predict('아몬드가 죽으면')

tier_predict('아이언맨')

tier_predict('정보통신기술협회')

