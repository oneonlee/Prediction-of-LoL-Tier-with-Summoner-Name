# -*- coding: utf-8 -*-
"""Prediction-of-LoL-Tier-with-Summoner-Name.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/104zlxJKSj_rerrvwoYVwBpM7BD7wEusL

# 리그오브레전드 소환사 이름으로 티어 예측하기

## 필요 라이브러리 설치 및 불러오기

hgtk : https://github.com/bluedisk/hangul-toolkit
"""

# 한글 자모 단위 처리 패키지 설치
!pip install hgtk

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from tqdm import tqdm
import random
import hgtk

"""## 데이터 내려받기 및 티어 나누기"""

lol_raw_data

"""## EDA (Exploratory Data Analysis, 탐색적 데이터 분석)

`lol_raw_data`의 결측치 확인
"""

lol_raw_data.isnull().sum()

lol_raw_data.info()

print('소환사 이름의 최대 길이 :{}'.format(max(len(sample) for sample in lol_raw_data['nickname'])))
print('소환사 이름의 평균 길이 :{}'.format(sum(map(len, lol_raw_data['nickname']))/len(lol_raw_data['nickname'])))
print()

plt.figure(dpi=150)
plt.hist([len(sample) for sample in lol_raw_data['nickname']], bins=50)
plt.xlabel('length of nicknames')
plt.ylabel('frequencies')
plt.show()

sns.set(font_scale = 1.5)
fig, axe = plt.subplots(ncols=1)
fig.set_size_inches(16,8)
sns.countplot(x = lol_raw_data['tier'])

unique_elements, counts_elements = np.unique(lol_raw_data['tier'], return_counts=True)

for i in range(len(unique_elements)):
  print(f"{unique_elements[i]} tier : {counts_elements[i]}")

"""'master' 이상의 티어는 하나로 묶어서 다시 살펴보기"""

lol_edited_data = lol_raw_data.replace({'tier' : 'challenger'}, 'above_master')
lol_edited_data = lol_edited_data.replace({'tier' : 'grandmaster'}, 'above_master') 
lol_edited_data = lol_edited_data.replace({'tier' : 'master'}, 'above_master')

sns.set(font_scale = 1.5)
fig, axe = plt.subplots(ncols=1)
fig.set_size_inches(16,8)
sns.countplot(x = lol_edited_data['tier'])

edited_tier_list, counts_elements = np.unique(lol_edited_data['tier'], return_counts=True)

for i in range(len(edited_tier_list)):
  print(f"{edited_tier_list[i]} tier : {counts_elements[i]}")

"""## Over Sampling 수행"""

minimum = min(counts_elements)
min_idx = np.where(counts_elements==minimum)
least_tier = edited_tier_list[min_idx][0]

over_sampled_data = lol_edited_data[lol_edited_data['tier'] == least_tier]

over_sampled_data1 = over_sampled_data.copy()
for i in range(len(over_sampled_data1)):
    over_sampled_data1['nickname'][i] = over_sampled_data1['nickname'][i][1:]

over_sampled_data2 = over_sampled_data.copy()
for i in range(len(over_sampled_data2)):
    over_sampled_data2['nickname'][i] = over_sampled_data2['nickname'][i][:-1]

over_sampled_data = pd.concat([over_sampled_data, over_sampled_data1])
over_sampled_data = pd.concat([over_sampled_data, over_sampled_data2])

edited_tier_list

maximum = len(over_sampled_data)
edited_tier_list = np.delete(edited_tier_list, min_idx)

for tier in edited_tier_list:
  over_sampled_data = pd.concat([over_sampled_data, lol_edited_data[lol_edited_data['tier'] == tier].sample(maximum)])

over_sampled_data

X_data = over_sampled_data['nickname']
y_data = over_sampled_data['tier']

"""## LSTM으로 티어 예측하기"""

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, GRU, Bidirectional, Embedding
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import load_model

vocab_size = 1000
max_len = 16

"""티어의 정수 인코딩"""

y_data = y_data.replace('above_master', 0)
y_data = y_data.replace('diamond', 1)
y_data = y_data.replace('platinum', 2)
y_data = y_data.replace('gold', 3)
y_data = y_data.replace('silver', 4)
y_data = y_data.replace('bronze', 5)
y_data = y_data.replace('iron', 6)

"""### 자음 모음 단위 토큰화"""

def word_to_jamo(token):
  def to_special_token(jamo):
    if not jamo:
      return '-'
    else:
      return jamo

  decomposed_token = ''
  for char in token:
    try:
      # char(음절)을 초성, 중성, 종성으로 분리
      cho, jung, jong = hgtk.letter.decompose(char)

      # 자모가 빈 문자일 경우 특수문자 -로 대체
      cho = to_special_token(cho)
      jung = to_special_token(jung)
      jong = to_special_token(jong)
      decomposed_token = decomposed_token + cho + jung + jong

    # 만약 char(음절)이 한글이 아닐 경우 자모를 나누지 않고 추가
    except Exception as exception:
      if type(exception).__name__ == 'NotHangulException':
        decomposed_token += char
    
  # 단어 토큰의 자모 단위 분리 결과를 추가
  return decomposed_token

def jamo_to_word(jamo_sequence):
  tokenized_jamo = []
  index = 0
  
  # 1. 초기 입력
  # jamo_sequence = 'ㄴㅏㅁㄷㅗㅇㅅㅐㅇ'

  while index < len(jamo_sequence):
    # 문자가 한글(정상적인 자모)이 아닐 경우
    if not hgtk.checker.is_hangul(jamo_sequence[index]):
      tokenized_jamo.append(jamo_sequence[index])
      index = index + 1

    # 문자가 정상적인 자모라면 초성, 중성, 종성을 하나의 토큰으로 간주.
    else:
      tokenized_jamo.append(jamo_sequence[index:index + 3])
      index = index + 3

  # 2. 자모 단위 토큰화 완료
  # tokenized_jamo : ['ㄴㅏㅁ', 'ㄷㅗㅇ', 'ㅅㅐㅇ']
  
  word = ''
  try:
    for jamo in tokenized_jamo:

      # 초성, 중성, 종성의 묶음으로 추정되는 경우
      if len(jamo) == 3:
        if jamo[2] == "-":
          # 종성이 존재하지 않는 경우
          word = word + hgtk.letter.compose(jamo[0], jamo[1])
        else:
          # 종성이 존재하는 경우
          word = word + hgtk.letter.compose(jamo[0], jamo[1], jamo[2])
      # 한글이 아닌 경우
      else:
        word = word + jamo

  # 복원 중(hgtk.letter.compose) 에러 발생 시 초기 입력 리턴.
  # 복원이 불가능한 경우 예시) 'ㄴ!ㅁㄷㅗㅇㅅㅐㅇ'
  except Exception as exception:  
    if type(exception).__name__ == 'NotHangulException':
      return jamo_sequence

  # 3. 단어로 복원 완료
  # word : '남동생'

  return word

max_len = 16*3

characters_data = {}
jamorized_X_data = []

for X in tqdm(X_data):
    # 자음 모음 단위 토큰화
    jamo_X = list(word_to_jamo(X))
    result = []

    for jamo in jamo_X: 
        result.append(jamo)
        if jamo not in characters_data:
            characters_data[jamo] = 0 
        characters_data[jamo] += 1
    jamorized_X_data.append(result) 


print('단어 집합 :', characters_data)

data_vocab_sorted = sorted(characters_data.items(), key = lambda x:x[1], reverse = True)
print(data_vocab_sorted)

data_word_to_index = {}
i = 0
for (word, frequency) in data_vocab_sorted :
    if frequency > 1 : # 빈도수가 작은 단어는 제외.
        i = i + 1
        data_word_to_index[word] = i

print(data_word_to_index)

data_word_to_index['OOV'] = len(data_word_to_index) + 1
print(data_word_to_index)

encoded_X_data = []
for jamorized_X in tqdm(jamorized_X_data):
    encoded_sentence = []
    for jamo in jamorized_X:
        try:
            # 단어 집합에 있는 단어라면 해당 단어의 정수를 리턴.
            encoded_sentence.append(data_word_to_index[jamo])
        except KeyError:
            # 만약 단어 집합에 없는 단어라면 'OOV'의 정수를 리턴.
            encoded_sentence.append(data_word_to_index['OOV'])
    encoded_X_data.append(encoded_sentence)

encoded_X_data = pad_sequences(encoded_X_data, maxlen=max_len)

X_train, X_test, y_train, y_test = train_test_split(encoded_X_data, y_data, test_size=0.33, random_state=815, stratify=y_data)

print(f"전체 data의 개수 : {len(encoded_X_data)}")
print(f"train data의 개수 : {len(X_train)}")
print(f"test data의 개수 : {len(X_test)}")

"""#### LSTM ver.1 - *0.21945*"""

embedding_dim = 128
hidden_units = 128
num_classes = len(set(y_train))

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(LSTM(hidden_units))
model.add(Dense(num_classes, activation='softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('jamo_best_model_lstm_128.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, batch_size=128, epochs=30, 
                    callbacks=[es, mc],
                    validation_data=(X_test, y_test))

"""#### LSTM ver.2 - *0.21972*"""

embedding_dim = 64
hidden_units = 64
num_classes = len(set(y_train))

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(LSTM(hidden_units))
model.add(Dense(num_classes, activation='softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('jamo_best_model_lstm_64.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, batch_size=64, epochs=30, 
                    callbacks=[es, mc],
                    validation_data=(X_test, y_test))

"""#### Bi-LSTM ver.1 *0.22147*"""

embedding_dim = 128
hidden_units = 128
num_classes = len(set(y_train))

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(Bidirectional(LSTM(hidden_units)))
model.add(Dense(num_classes, activation='softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('jamo_best_model_bilstm_128.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, batch_size=128, epochs=30, 
                    callbacks=[es, mc],
                    validation_data=(X_test, y_test))

"""#### Bi-LSTM ver.2 *0.22132*"""

embedding_dim = 64
hidden_units = 64
num_classes = len(set(y_train))

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(Bidirectional(LSTM(hidden_units)))
model.add(Dense(num_classes, activation='softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('jamo_best_model_bilstm_64.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, batch_size=64, epochs=30, 
                    callbacks=[es, mc],
                    validation_data=(X_test, y_test))

"""#### GRU ver.1 *0.22105*"""

embedding_dim = 128
hidden_units = 128
num_classes = len(set(y_train))

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(GRU(hidden_units))
model.add(Dense(num_classes, activation='softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('jamo_best_model_gru_128.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, batch_size=128, epochs=30, 
                    callbacks=[es, mc],
                    validation_data=(X_test, y_test))

"""#### GRU ver.2 *0.21993*"""

embedding_dim = 64
hidden_units = 64
num_classes = len(set(y_train))

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(GRU(hidden_units))
model.add(Dense(num_classes, activation='softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('jamo_best_model_gru_64.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, batch_size=64, epochs=30, 
                    callbacks=[es, mc],
                    validation_data=(X_test, y_test))

"""#### Bi-GRU ver.1 *0.22192*"""

embedding_dim = 128
hidden_units = 128
num_classes = len(set(y_train))

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(Bidirectional(GRU(hidden_units)))
model.add(Dense(num_classes, activation='softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('jamo_best_model_biGRU_128.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, batch_size=128, epochs=30, 
                    callbacks=[es, mc],
                    validation_data=(X_test, y_test))

"""#### Bi-GRU ver.2 *0.22210*"""

embedding_dim = 64
hidden_units = 64
num_classes = len(set(y_train))

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(Bidirectional(GRU(hidden_units)))
model.add(Dense(num_classes, activation='softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('jamo_best_model_biGRU_64.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, batch_size=64, epochs=30, 
                    callbacks=[es, mc],
                    validation_data=(X_test, y_test))

"""### 모델 predict

가장 성능이 좋은 model을 `load` 한다.
"""

model = load_model('jamo_best_model_biGRU_64.h5')

def tier_predict(nickname):
    jamo_nickname = word_to_jamo(nickname)
    encoded_nickname = []
    for jamo in jamo_nickname:
        try:
            # 단어 집합에 있는 단어라면 해당 단어의 정수를 리턴.
            encoded_nickname.append(data_word_to_index[jamo])
        except KeyError:
            # 만약 단어 집합에 없는 단어라면 'OOV'의 정수를 리턴.
            encoded_nickname.append(data_word_to_index['OOV'])
    padded_nickname = pad_sequences([encoded_nickname], maxlen=16)
    
    tier_index = np.argmax(model.predict(padded_nickname))
    tier_confidence = np.max(model.predict(padded_nickname))
    tier_list = ['마스터 이상', '다이아몬드', '플래티넘', '골드', '실버', '브론즈', '아이언']

    return tier_list[tier_index], tier_confidence

tier_predict('KT way')